"""
RAG Agent for Maccabi AI Orchestrator
Retrieves relevant documents and generates answers using Claude.
"""

import os
from typing import Any

import anthropic
from dotenv import load_dotenv

from tools.vectordb import search_similar

load_dotenv()

# =============================================================================
# CONFIGURATION
# =============================================================================

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
MODEL = "claude-sonnet-4-20250514"


# =============================================================================
# RAG AGENT
# =============================================================================

def create_rag_prompt(query: str, context_docs: list[dict]) -> str:
    """Create a prompt with retrieved context for Claude."""
    
    # Format retrieved documents
    context_parts = []
    for i, doc in enumerate(context_docs, 1):
        context_parts.append(f"""
××¡××š {i} (××§×•×¨: {doc['source_file']}, ×¨×œ×•×•× ×˜×™×•×ª: {doc['similarity']:.0%}):
{doc['content']}
""")
    
    context_text = "\n---\n".join(context_parts)
    
    prompt = f"""××ª×” ×¢×•×–×¨ ×•×™×¨×˜×•××œ×™ ×©×œ ××›×‘×™ ×©×™×¨×•×ª×™ ×‘×¨×™××•×ª. ×¢×œ×™×š ×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª ×”××‘×•×˜×—×™× ×‘×¢×‘×¨×™×ª, ×‘×¦×•×¨×” ××§×¦×•×¢×™×ª ×•××“×™×‘×”.

×”×©×ª××© ×‘××™×“×¢ ×”×‘× ××ª×•×š ××¡××›×™ ××›×‘×™ ×›×“×™ ×œ×¢× ×•×ª ×¢×œ ×”×©××œ×”:

{context_text}

---

×©××œ×ª ×”××‘×•×˜×—: {query}

×”× ×—×™×•×ª:
1. ×¢× ×” ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“
2. ×”×ª×‘×¡×¡ ×¨×§ ×¢×œ ×”××™×“×¢ ×©×¡×•×¤×§ ×œ××¢×œ×”
3. ×× ×”××™×“×¢ ×œ× ××¡×¤×™×§ ×œ×ª×©×•×‘×” ××œ××”, ×¦×™×™×Ÿ ×–××ª ×‘×›× ×•×ª
4. ×”×™×” ×ª××¦×™×ª×™ ××š ××§×™×£
5. ×× ×¨×œ×•×•× ×˜×™, ×”×¤× ×” ××ª ×”××‘×•×˜×— ×œ××•×§×“ *3555 ×œ×¤×¨×˜×™× × ×•×¡×¤×™×

×ª×©×•×‘×”:"""

    return prompt


def rag_query(query: str, top_k: int = 10) -> dict[str, Any]:
    """
    Execute a RAG query: retrieve documents and generate answer.
    
    Args:
        query: User's question in Hebrew
        top_k: Number of documents to retrieve
    
    Returns:
        Dict with answer, sources, and metadata
    """
    # Step 1: Retrieve relevant documents
    retrieved_docs = search_similar(query, top_k=top_k)
    
    if not retrieved_docs:
        return {
            "answer": "××¦×˜×¢×¨, ×œ× ××¦××ª×™ ××™×“×¢ ×¨×œ×•×•× ×˜×™ ×œ×©××œ×ª×š. ×× × ×¤× ×” ×œ××•×§×“ ××›×‘×™ *3555.",
            "sources": [],
            "query": query
        }
    
    # Step 2: Create prompt with context
    prompt = create_rag_prompt(query, retrieved_docs)
    
    # Step 3: Generate answer with Claude
    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
    
    response = client.messages.create(
        model=MODEL,
        max_tokens=1024,
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    
    answer = response.content[0].text
    
    # Step 4: Return structured response
    return {
        "answer": answer,
        "sources": [
            {
                "file": doc["source_file"],
                "similarity": doc["similarity"],
                "snippet": doc["content"][:200] + "..."
            }
            for doc in retrieved_docs
        ],
        "query": query,
        "model": MODEL,
        "tokens_used": response.usage.input_tokens + response.usage.output_tokens
    }


# =============================================================================
# LANGGRAPH NODE FUNCTION
# =============================================================================

def rag_agent_node(state: dict) -> dict:
    """
    LangGraph node function for RAG agent.
    
    Expected state:
        - query: str (user's question)
    
    Returns updated state with:
        - rag_response: dict (answer + sources)
    """
    query = state.get("query", "")
    
    if not query:
        return {
            **state,
            "rag_response": {
                "answer": "×œ× ×”×ª×§×‘×œ×” ×©××œ×”.",
                "sources": [],
                "query": ""
            }
        }
    
    result = rag_query(query)
    
    return {
        **state,
        "rag_response": result
    }


# =============================================================================
# CLI INTERFACE
# =============================================================================

def interactive_mode():
    """Run interactive Q&A session."""
    print("\n" + "=" * 60)
    print("ğŸ¥ ××›×‘×™ AI - ××¢×¨×›×ª ×©××œ×•×ª ×•×ª×©×•×‘×•×ª")
    print("=" * 60)
    print("×”×§×œ×“ ×©××œ×” ×‘×¢×‘×¨×™×ª (××• 'exit' ×œ×™×¦×™××”)\n")
    
    while True:
        query = input("â“ ×©××œ×”: ").strip()
        
        if query.lower() in ['exit', 'quit', '×™×¦×™××”']:
            print("\nğŸ‘‹ ×œ×”×ª×¨××•×ª!")
            break
        
        if not query:
            continue
        
        print("\nğŸ” ××—×¤×© ××™×“×¢ ×¨×œ×•×•× ×˜×™...")
        result = rag_query(query)
        
        print("\n" + "-" * 40)
        print("ğŸ’¬ ×ª×©×•×‘×”:")
        print(result["answer"])
        print("-" * 40)
        
        print("\nğŸ“š ××§×•×¨×•×ª:")
        for src in result["sources"]:
            print(f"  â€¢ {src['file']} ({src['similarity']:.0%})")
        
        print(f"\nğŸ“Š ×˜×•×§× ×™×: {result['tokens_used']}")
        print()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Maccabi RAG Agent")
    parser.add_argument(
        "--query",
        type=str,
        help="Single query to answer"
    )
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="Run interactive Q&A session"
    )
    
    args = parser.parse_args()
    
    if args.interactive:
        interactive_mode()
    elif args.query:
        result = rag_query(args.query)
        print(f"\nğŸ’¬ ×ª×©×•×‘×”:\n{result['answer']}")
        print(f"\nğŸ“š ××§×•×¨×•×ª: {[s['file'] for s in result['sources']]}")
    else:
        # Default: interactive mode
        interactive_mode()